model_name: "distilbert-base-uncased"
device: "auto"
seed: 42

# Treinamento
batch_size: 32
learning_rate: 1e-4
epochs: 3
max_grad_norm: 1.0
warmup_ratio: 0.1

# LoRA
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05

# Regularização
lambda_ortho: 0.1
lambda_ewc: 100.0

# Replay gerativo
replay_ratio: 0.2
generation_model: "gpt2"
max_gen_length: 50
temperature: 0.7
top_p: 0.9

# Componentes (flags booleanas)
use_ewc: true
use_orthogonal: true
use_replay: true
use_lateral: false

# Checkpointing
checkpoint_every: 1000
keep_last_n_checkpoints: 3
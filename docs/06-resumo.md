# Resumo

O estudo investiga aprendizado contínuo em Processamento de Linguagem Natural, com foco na mitigação do esquecimento catastrófico ao treinar modelos em sequências de tarefas. Parte-se do dilema estabilidade–plasticidade e do contexto prático em que dados antigos não podem ser armazenados integralmente, seja por custo, privacidade ou políticas de uso. Nessa conjuntura, busca-se preservar desempenho em tarefas anteriores enquanto se mantém capacidade de adaptação a novas distribuições, com uso parcimonioso de parâmetros e de computação. Propõe-se uma arquitetura híbrida que combina modularização progressiva inspirada em redes neurais progressivas, adaptações de baixo ranque com restrição ortogonal, consolidação elástica de pesos e replay gerativo sem retenção de dados brutos, visando equilibrar plasticidade e estabilidade com eficiência paramétrica. A abordagem emprega modelos base de porte moderado, com adaptadores LoRA específicos por tarefa e ortogonalidade para separar subespaços de atualização; aplica EWC a componentes compartilhados parcialmente destravados; e intercala lotes sintéticos para recuperar conhecimentos prévios. O protocolo experimental contempla uma sequência de tarefas de PLN com reavaliação cumulativa após cada etapa, empregando métricas como acurácia média, taxa de esquecimento, transferências forward e backward, crescimento paramétrico e custo computacional, além de ablações para isolar o efeito de cada mecanismo. Espera-se demonstrar redução do esquecimento sem degradar a plasticidade, oferecendo ganhos marginais mensuráveis quando cada componente é ativado, com sobrecusto paramétrico moderado por tarefa. Espera-se como contribuição principal um arranjo integrado, original e eficiente em parâmetros para aprendizado contínuo em PLN, acompanhado de um protocolo reprodutível e de diretrizes de engenharia.

## Palavras-chave

Aprendizado Contínuo; LoRA ortogonal; EWC; Replay Gerativo; PLN

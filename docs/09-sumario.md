# Sumário

## Elementos Pré-textuais

1. Capa
2. Folha de rosto
3. Folha de aprovação / depósito
4. Dedicatória
5. Agradecimentos
6. Resumo
7. Abstract
8. Listas
   - 8.1 Lista de Figuras
   - 8.2 Lista de Tabelas
   - 8.3 Lista de Algoritmos
   - 8.4 Lista de Abreviaturas e Siglas
9. Sumário

## Elementos Textuais

### Capítulo 1 - Introdução
1.1 Contextualização e motivação
1.2 Problema de pesquisa
1.3 Objetivos gerais e específicos
1.4 Justificativa
1.5 Principais contribuições
1.6 Estrutura do trabalho

### Capítulo 2 - Referencial Teórico
2.1 Fundamentação teórica sobre aprendizado contínuo em PLN
2.2 Redes Neurais Progressivas (PNN)
2.3 Adaptações de Baixo Ranque com Restrições Ortogonais (LoRA e O-LoRA)
2.4 Elastic Weight Consolidation (EWC)
2.5 Replay Gerativo
2.6 Métricas de avaliação em aprendizado contínuo
2.7 Trabalhos correlatos

### Capítulo 3 - Metodologia
3.1 Arquitetura proposta
3.2 Protocolo experimental
3.3 Fluxo de treinamento
3.4 Ambiente computacional
3.5 Protocolo de avaliação
3.6 Baselines e ablações

### Capítulo 4 - Resultados e Discussão
4.1 Resultados principais
4.2 Análise de eficiência
4.3 Análise de ablação
4.4 Discussão dos resultados
4.5 Limitações identificadas

### Capítulo 5 - Conclusão
5.1 Síntese dos resultados
5.2 Contribuições alcançadas
5.3 Limitações do estudo
5.4 Trabalhos futuros

## Elementos Pós-textuais

Referências bibliográficas

Apêndices
- Apêndice A: Configurações completas de hiperparâmetros
- Apêndice B: Pseudocódigo dos algoritmos principais
- Apêndice C: Tabelas detalhadas de resultados por tarefa
- Apêndice D: Análise adicional de ablação
- Apêndice E: Instruções de reprodução dos experimentos

Anexos
- Anexo A: Código-fonte principal
- Anexo B: Scripts de experimentos
- Anexo C: Configurações YAML
- Anexo D: Logs de treinamento (amostras)

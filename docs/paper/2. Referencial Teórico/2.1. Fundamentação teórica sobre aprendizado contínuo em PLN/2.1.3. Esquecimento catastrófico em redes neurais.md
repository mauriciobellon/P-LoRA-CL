# 2.1.3. Esquecimento catastrófico em redes neurais

O esquecimento catastrófico ocorre quando o treinamento em uma nova tarefa altera parâmetros que eram críticos para tarefas anteriores. Em redes neurais profundas, onde milhões de parâmetros são compartilhados entre diferentes camadas e funções, essa interferência pode ocorrer em múltiplos níveis. O problema é especialmente pronunciado quando tarefas novas e antigas requerem atualizações conflitantes dos mesmos parâmetros.

A natureza do esquecimento catastrófico em Transformers é particularmente complexa devido à arquitetura de atenção e às múltiplas camadas de representação. Parâmetros compartilhados em embeddings, camadas de atenção e redes feed-forward podem ser afetados de forma diferente dependendo de como as tarefas utilizam essas representações. Compreender e mitigar essas interferências requer abordagens que identifiquem e protejam parâmetros críticos ou que isolam atualizações para diferentes tarefas.

Entre os mecanismos de mitigação, destacam-se: (i) isolamento estrutural via PNNs (Rusu et al., 2016); (ii) regularização baseada em informação via EWC (Kirkpatrick et al., 2017; Schwarz et al., 2018); (iii) replay (Shin et al., 2017), inclusive sem dados brutos; e (iv) restrições ortogonais sobre atualizações/espelhos de parâmetros (Zeng et al., 2019; Farajtabar et al., 2019).

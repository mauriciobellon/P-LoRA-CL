# 2.1.1. Definição e desafios do aprendizado contínuo

O aprendizado contínuo, também referido como continual learning ou lifelong learning, consiste na capacidade de um sistema de inteligência artificial aprender novas tarefas de forma sequencial, acumulando conhecimento ao longo do tempo sem esquecer o que foi aprendido anteriormente. Diferentemente do fine-tuning tradicional, onde um conjunto de dados é fixo e o modelo é ajustado apenas uma vez, no aprendizado contínuo o modelo enfrenta desafios significativos ao adaptar-se a distribuições que evoluem sem reiniciar o treinamento do zero para cada novo contexto.

Panoramas recentes oferecem taxonomias abrangentes de métodos (regularização, arquitetura, replay) e protocolos de avaliação, destacando dificuldades de comparação e a necessidade de métricas padronizadas (De Lange et al., 2021; Parisi et al., 2019).

O principal desafio é o esquecimento catastrófico — a tendência de redes neurais esquecerem abruptamente tarefas antigas ao aprender tarefas novas. Esse fenômeno foi documentado desde os primórdios das redes neurais e permanece um obstáculo central em sistemas adaptativos. Uma arquitetura de aprendizado contínuo deve equilibrar plasticidade (para incorporar novos conhecimentos) e estabilidade (para reter conhecimentos prévios), evitando que gradientes das tarefas recentes sobrescrevam parâmetros importantes das tarefas passadas.

Métricas clássicas para quantificar retenção e transferência incluem Acurácia Média (ACC), Backward Transfer (BWT), Forward Transfer (FWT) e Forgetting, popularizadas por Lopez-Paz & Ranzato (2017) e sistematizadas por van de Ven & Tolias (2019).

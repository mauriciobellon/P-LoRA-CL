# 2.1.2. Dilema estabilidade-plasticidade

O dilema estabilidade-plasticidade, formalizado por Grossberg (1987), é fundamental para entender os desafios do aprendizado contínuo. Plasticidade refere-se à capacidade do sistema de modificar seus parâmetros para incorporar novos conhecimentos, enquanto estabilidade refere-se à capacidade de preservar conhecimentos previamente adquiridos. Em redes neurais tradicionais, essas capacidades estão em conflito: aumentar a plasticidade facilita o aprendizado de novas tarefas mas aumenta a vulnerabilidade ao esquecimento, enquanto aumentar a estabilidade protege contra esquecimento mas pode impedir adaptação efetiva a novas distribuições.

O equilíbrio ótimo depende de múltiplos fatores, incluindo a similaridade entre tarefas, a capacidade do modelo, e o regime de treinamento. Em PLN, onde tarefas podem variar desde classificação de sentimento até tradução ou sumarização, encontrar esse equilíbrio é particularmente desafiador devido à diversidade de objetivos e distribuições.

Na prática, estratégias distintas caminham ao longo desse trade-off:
- Isolamento estrutural (PNN): maximiza estabilidade ao congelar colunas passadas, sacrificando eficiência paramétrica;
- Regularização baseada em importância (EWC/MAS/SI): ancora pesos críticos com penalidades quadráticas (Huszár, 2018 discute limitações), preservando estabilidade moderada e com pouco overhead; 
- Replay (real ou gerativo): restaura gradientes úteis do passado para manter trajetórias de solução em regiões compatíveis com tarefas anteriores; 
- Projeções/ortogonalidade: força atualizações a evitarem subespaços usados (OGD, OWM) ou aloca subespaços LoRA quase-ortogonais para tarefas sucessivas, mitigando interferência direta.

Matematicamente, a interferência entre tarefas pode ser aproximada via similaridade de gradientes (e.g., cosseno entre ∇_θ L_i e ∇_θ L_j). Cossenos negativos indicam conflitos (update em T_k degrada T_j), enquanto positivos sugerem transferência. Técnicas de estabilidade-plasticidade eficazes buscam aumentar a probabilidade de cossenos não-negativos ao longo da sequência, seja pela proteção de parâmetros importantes (EWC), pelo reuso controlado de representações (PNN, conexões laterais), pelo reforço de dados (replay), ou pela redução explícita da sobreposição entre subespaços de atualização (ortogonalidade).

# 2.1.1. Definição e desafios do aprendizado contínuo

O aprendizado contínuo, também referido como continual learning ou lifelong learning, consiste na capacidade de um sistema de inteligência artificial aprender novas tarefas de forma sequencial, acumulando conhecimento ao longo do tempo sem esquecer o que foi aprendido anteriormente. Diferentemente do fine-tuning tradicional, onde um conjunto de dados é fixo e o modelo é ajustado apenas uma vez, no aprendizado contínuo o modelo enfrenta desafios significativos ao adaptar-se a distribuições que evoluem sem reiniciar o treinamento do zero para cada novo contexto.

Panoramas recentes oferecem taxonomias abrangentes de métodos (regularização, arquitetura, replay) e protocolos de avaliação, destacando dificuldades de comparação e a necessidade de métricas padronizadas (De Lange et al., 2021; Parisi et al., 2019).

O principal desafio é o esquecimento catastrófico — a tendência de redes neurais esquecerem abruptamente tarefas antigas ao aprender tarefas novas. Esse fenômeno foi documentado desde os primórdios das redes neurais e permanece um obstáculo central em sistemas adaptativos. Uma arquitetura de aprendizado contínuo deve equilibrar plasticidade (para incorporar novos conhecimentos) e estabilidade (para reter conhecimentos prévios), evitando que gradientes das tarefas recentes sobrescrevam parâmetros importantes das tarefas passadas.

Métricas clássicas para quantificar retenção e transferência incluem Acurácia Média (ACC), Backward Transfer (BWT), Forward Transfer (FWT) e Forgetting, popularizadas por Lopez-Paz & Ranzato (2017) e sistematizadas por van de Ven & Tolias (2019).

Além da definição geral, é útil distinguir cenários e protocolos: (i) task-incremental (as fronteiras entre tarefas são conhecidas e disponíveis na inferência), (ii) class-incremental (as classes acumulam ao longo do tempo e a inferência é task-agnostic) e (iii) domain-incremental (a tarefa permanece a mesma, mas a distribuição muda) (van de Ven & Tolias, 2019). Esses cenários impõem graus distintos de dificuldade e influenciam quais mecanismos (arquiteturais, de regularização ou de replay) tendem a funcionar melhor.

Em PLN, o aprendizado contínuo frequentemente ocorre sob concept drift e domain shift (novos tópicos, estilos, gírias, ou mudanças no registro linguístico), o que exige que o modelo incorpore novas regularidades sem perder competências linguísticas e pragmáticas adquiridas. Restrições práticas — privacidade, compliance regulatório, e limites de armazenamento — também afetam a escolha entre métodos que dependem de buffers de dados (rehearsal) versus alternativas gerativas ou puramente paramétricas (EWC/MAS/SI). Para comparações justas, recomenda-se explicitar cenários, conhecimento de tarefa na inferência (task-aware vs task-agnostic), orçamento de memória e custos computacionais, evitando conclusões enviesadas por protocolos (De Lange et al., 2021).

Por fim, diferentes famílias de soluções endereçam o problema por ângulos complementares: (a) isolamento estrutural (PNN), (b) proteção de parâmetros importantes via regularização (EWC, MAS, SI), (c) reforço de dados por rehearsal real ou gerativo (GEM/A-GEM; LAMOL), e (d) controle de interferência por otimização/projeção (OGD/OWM) e por alocação de subespaços ortogonais em adaptações de baixo ranque (LoRA com alocação ortogonal). O presente trabalho investiga sinergias entre essas frentes no contexto de PLN.
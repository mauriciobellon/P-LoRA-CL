# 2.5.2. LAMOL: Language Modeling for Lifelong Learning

Em PLN, o LAMOL (Language Modeling for Lifelong Learning), proposto por Sun et al. (2020), exemplifica bem o replay gerativo. Nele, um único modelo de linguagem é treinado para duas funções simultâneas: (i) resolver a tarefa atual e (ii) gerar dados de tarefas anteriores sob forma de texto. O processo funciona assim: antes (ou durante) de treinar na tarefa T_k, o modelo gera um conjunto de exemplos fictícios das tarefas T_1, ..., T_{k-1} que já aprendeu.

Esses exemplos gerados — às vezes chamados de exemplos "nostálgicos" — são então misturados com os dados reais da nova tarefa durante o treino de T_k. O gradiente que atualiza o modelo é influenciado não só pela nova tarefa, mas também por recriações das antigas, reforçando as conexões relevantes para o desempenho passado.

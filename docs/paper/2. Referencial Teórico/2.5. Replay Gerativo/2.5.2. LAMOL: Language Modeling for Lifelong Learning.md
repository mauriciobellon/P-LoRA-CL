# 2.5.2. LAMOL: Language Modeling for Lifelong Learning

Em PLN, o LAMOL (Language Modeling for Lifelong Learning), proposto por Sun et al. (2020), exemplifica bem o replay gerativo. Nele, um único modelo de linguagem é treinado para duas funções simultâneas: (i) resolver a tarefa atual e (ii) gerar dados de tarefas anteriores sob forma de texto. O processo funciona assim: antes (ou durante) de treinar na tarefa T_k, o modelo gera um conjunto de exemplos fictícios das tarefas T_1, ..., T_{k-1} que já aprendeu.

Esses exemplos gerados — às vezes chamados de exemplos "nostálgicos" — são então misturados com os dados reais da nova tarefa durante o treino de T_k. O gradiente que atualiza o modelo é influenciado não só pela nova tarefa, mas também por recriações das antigas, reforçando as conexões relevantes para o desempenho passado.

Um aspecto prático do LAMOL é o uso de tokens especiais para condicionar a geração por tarefa/objetivo, permitindo ao mesmo LM atuar tanto como solucionador quanto como gerador condicionado. O treino envolve uma mistura controlada de perdas (resolução vs. LM gerativo), e a proporção de exemplos gerados por tarefa influencia a estabilidade: proporções maiores reforçam a memória, mas aumentam o custo e podem enviesar o treino atual. Seleção/curadoria dos prompts de geração é igualmente importante para evitar deriva de distribuição nas amostras sintéticas.

# 2.2.1. Conceito e arquitetura básica

As Redes Neurais Progressivas (Progressive Neural Networks - PNN), proposta por Rusu et al. (2016), constituem uma abordagem baseada em arquitetura para aprendizado contínuo. A ideia central é expandir a capacidade da rede incrementalmente a cada nova tarefa, adicionando um novo conjunto de neurônios (uma nova "coluna" ou módulo de rede) específico para cada tarefa aprendida.

Quando a tarefa T_k é iniciada, cria-se uma nova coluna de parâmetros inicializada (geralmente a partir de uma versão pré-treinada) para aprender T_k. As colunas das tarefas anteriores são congeladas — seus pesos não são alterados durante o treinamento de T_k — e são estabelecidas conexões laterais da saída (ou camadas intermediárias) de cada coluna anterior para a nova coluna. Essas conexões laterais permitem que a nova coluna reutilize e transfira conhecimento das características previamente aprendidas nas tarefas anteriores, promovendo transferência de aprendizado para frente.

Na prática, PNNs particionam o problema de estabilidade-plasticidade pela construção: estabilidade é garantida pois as colunas antigas não sofrem atualização; plasticidade é alocada à coluna nova. As conexões laterais (skip/concat/atenção) fornecem um caminho explícito para transferência positiva sem risco de degradar colunas antigas. O custo é paramétrico: cada tarefa adiciona aproximadamente 100% dos parâmetros da arquitetura de base, além do custo das conexões laterais, o que inviabiliza PNNs puras em modelos de grande porte sem medidas complementares de compressão.

# 2.2.1. Conceito e arquitetura básica

As Redes Neurais Progressivas (Progressive Neural Networks - PNN), proposta por Rusu et al. (2016), constituem uma abordagem baseada em arquitetura para aprendizado contínuo. A ideia central é expandir a capacidade da rede incrementalmente a cada nova tarefa, adicionando um novo conjunto de neurônios (uma nova "coluna" ou módulo de rede) específico para cada tarefa aprendida.

Quando a tarefa T_k é iniciada, cria-se uma nova coluna de parâmetros inicializada (geralmente a partir de uma versão pré-treinada) para aprender T_k. As colunas das tarefas anteriores são congeladas — seus pesos não são alterados durante o treinamento de T_k — e são estabelecidas conexões laterais da saída (ou camadas intermediárias) de cada coluna anterior para a nova coluna. Essas conexões laterais permitem que a nova coluna reutilize e transfira conhecimento das características previamente aprendidas nas tarefas anteriores, promovendo transferência de aprendizado para frente.

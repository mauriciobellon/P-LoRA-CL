# 2.6.4. Forward Transfer

O Forward Transfer complementa o BWT medindo a influência que o conhecimento das tarefas anteriores exerce sobre o aprendizado de tarefas futuras. Indica se o modelo aprendeu a aprender: se tarefas passadas fornecem representações ou parâmetros que facilitam a obtenção de melhor desempenho em tarefas novas, mesmo antes de treiná-las extensivamente.

Formalmente, FWT é definido como a média de R_{i,j} para i<j (acurácia em tarefas futuras j antes de treiná-las), normalizada de forma que a contribuição de um classificador aleatório seja subtraída. Um FWT positivo significa que o modelo, por ter aprendido tarefas anteriores, já inicia melhor do que um modelo não treinado quando encontra uma tarefa nova, indicando transferência de conhecimento útil para frente.

Em PLN, FWT alto pode refletir reuso de conhecimento semântico e sintático comum (e.g., embeddings e primeiros blocos de Transformer), ou a presença de adapters que capturam elementos reutilizáveis entre tarefas. Reportar FWT por família de tarefa ajuda a diagnosticar quão generalizáveis são as representações aprendidas sob o protocolo contínuo.

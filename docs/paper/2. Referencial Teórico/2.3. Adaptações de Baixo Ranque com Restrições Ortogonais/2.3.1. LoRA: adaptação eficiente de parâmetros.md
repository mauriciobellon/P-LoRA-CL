# 2.3.1. LoRA: adaptação eficiente de parâmetros

O LoRA (Low-Rank Adaptation), proposto por Hu et al. (2021), oferece uma forma de adaptar modelos de linguagem de grande porte de maneira leve e modular. A técnica funciona congelando todos os pesos originais do modelo pré-treinado e injetando pequenos módulos treináveis de baixo ranque em cada camada relevante da arquitetura Transformer.

Para cada matriz de pesos W em camadas selecionadas (por exemplo, nas projeções de atenção ou na rede feed-forward), LoRA introduz duas matrizes menores A e B de dimensões de posto r (tipicamente r bem menor que o tamanho original da camada) de forma que a atualização da camada seja W + ΔW, onde ΔW = AB representa um ajuste de baixo ranque aprendido para a nova tarefa. Em vez de ajustar todos os pesos do modelo, apenas os parâmetros nesses pequenos módulos são aprendidos.

Na prática, LoRA é frequentemente aplicado às projeções de atenção (Q, K, V e/ou O) e, em alguns casos, às projeções internas do MLP. Hiperparâmetros usuais incluem o ranque r e um fator de escalonamento α (às vezes implementado via “lora_alpha”), além de dropout nos caminhos LoRA. Durante a inferência, os deltas podem ser mesclados em W (merge) sem aumentar a latência, ou aplicados como caminho paralelo (dependendo da implementação). Essa modularidade viabiliza manter um conjunto de adapters por tarefa e ativá-los sob demanda, o que é atraente para cenários de aprendizado contínuo com orçamento de memória restrito.

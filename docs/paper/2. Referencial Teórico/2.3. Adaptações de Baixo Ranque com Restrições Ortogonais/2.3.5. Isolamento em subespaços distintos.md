# 2.3.5. Isolamento em subespaços distintos

O isolamento em subespaços ortogonais atua como um análogo leve de uma PNN: em vez de dedicar uma coluna inteira de parâmetros para cada tarefa, dedica-se apenas um subespaço de adaptações de baixo ranque, mas assegurando que haja pouca sobreposição entre as direções de atualização de tarefas diferentes. Estudos reportam que métodos como O-LoRA efetivamente reduzem a interferência entre tarefas e, assim, o esquecimento, quando comparados ao uso ingênuo de LoRA sequencial.

O custo adicional do O-LoRA é marginal: os adaptadores ortogonais têm o mesmo número de parâmetros do LoRA convencional (apenas o procedimento de treinamento muda), mantendo a eficiência. No entanto, garantir ortogonalidade perfeita entre subespaços de diversas tarefas pode se tornar difícil conforme o número de tarefas cresce, especialmente se o ranque r for limitado.

Na prática, a ortogonalização pode ser implementada por (i) penalizações de correlação/cosseno entre colunas de A_i e bases prévias, (ii) projeções tipo Gram–Schmidt sobre bases acumuladas, ou (iii) seleção proativa de vetores-base com baixa sensibilidade à interferência (como proposto por PLAN). À medida que o número de tarefas cresce, pode ser necessário reciclar/compactar subespaços para manter a eficiência, por exemplo, reestimando bases partilhadas entre tarefas similares e reservando direções exclusivas para tarefas com maior conflito.

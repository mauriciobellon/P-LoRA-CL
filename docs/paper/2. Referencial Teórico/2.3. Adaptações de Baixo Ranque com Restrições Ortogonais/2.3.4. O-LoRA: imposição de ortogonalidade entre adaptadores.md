# 2.3.4. O-LoRA: imposição de ortogonalidade entre adaptadores

O LoRA puro não resolve por si só o esquecimento catastrófico quando usado sequencialmente. Se o mesmo conjunto de adaptadores for reutilizado para múltiplas tarefas em sequência, a tarefa nova pode sobrescrever as representações adquiridas pelos adaptadores na tarefa anterior. Para enfrentar essa limitação, métodos baseados em ortogonalidade impõem restrições entre os subespaços de adaptação de cada tarefa, reduzindo a sobreposição entre direções de atualização.

Ao treinar os adaptadores de uma nova tarefa, pode-se adicionar um termo de regularização (ou realizar projeções explícitas) para que os novos deltas de baixo ranque fiquem aproximadamente ortogonais aos espaços gerados pelos deltas das tarefas anteriores. Assim, cada tarefa T_i aprende sua atualização ΔW_i = A_i B_i em um subespaço linear distinto, minimizando projeções em direções usadas por ΔW_j de tarefas j < i. Essa ideia dialoga com técnicas de projeção ortogonal no espaço de gradientes/pesos (OGD, OWM) e com alocação proativa de bases ortogonais em subespaços LoRA para aprendizado contínuo, como em PLAN (Wang, Zhuang & Zhang, 2025).

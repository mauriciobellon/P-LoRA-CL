# 2.3.4. O-LoRA: imposição de ortogonalidade entre adaptadores

O LoRA puro não resolve por si só o esquecimento catastrófico quando usado sequencialmente. Se o mesmo conjunto de adaptadores for reutilizado para múltiplas tarefas em sequência, a tarefa nova pode sobrescrever as representações adquiridas pelos adaptadores na tarefa anterior. Para enfrentar essa limitação, o O-LoRA (Orthogonal LoRA), proposto por Wang et al. (2023), impõe restrições ortogonais entre os subespaços de adaptação de cada tarefa.

Ao treinar os adaptadores de uma nova tarefa, adiciona-se um termo de regularização (ou projeta-se explicitamente) para que os novos deltas de baixo ranque fiquem ortogonais aos espaços gerados pelos deltas das tarefas anteriores. Assim, cada tarefa T_i aprende sua atualização ΔW_i = A_i B_i em um subespaço linear distinto, minimizando projeções em direções usadas por ΔW_j de tarefas j < i.

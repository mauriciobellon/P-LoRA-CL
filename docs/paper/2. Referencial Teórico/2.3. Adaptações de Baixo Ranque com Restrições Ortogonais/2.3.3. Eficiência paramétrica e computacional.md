# 2.3.3. Eficiência paramétrica e computacional

Uma vantagem crucial do LoRA é a redução dramática de parâmetros ajustáveis. No caso extremo do GPT-3 (175 bilhões de parâmetros), estudos mostraram que é possível ajustar cerca de 18 milhões de parâmetros (via LoRA) para especializar o modelo em uma nova tarefa, mantendo desempenho similar ao fine-tuning completo — isso equivale a apenas ~0,01% dos parâmetros originais. Em modelos menores como BERT ou GPT-2, o overhead típico do LoRA por tarefa costuma ficar na faixa de 0,1% a 3% do total de parâmetros.

Além da eficiência paramétrica, o LoRA mantém eficiência computacional: como o modelo base permanece congelado e compartilhado entre todas tarefas, não há aumento de latência na inferência quando os deltas são mesclados de volta nos pesos originais. Isso torna viável carregar múltiplos adapters LoRA — um por tarefa — em memória sem explodir o uso de recursos.

Quando combinado com quantização (por exemplo, QLoRA; Dettmers et al., 2023), é possível reduzir substancialmente o custo de memória do modelo base sem sacrificar a capacidade de adaptação: o backbone quantizado permanece congelado e apenas os adapters de baixo ranque (em precisão maior) são aprendidos. Essa combinação é particularmente atraente em cenários de aprendizado contínuo, pois permite manter diversos adapters por tarefa com custo marginal, viabilizando catálogos de competências que podem ser ativadas seletivamente.

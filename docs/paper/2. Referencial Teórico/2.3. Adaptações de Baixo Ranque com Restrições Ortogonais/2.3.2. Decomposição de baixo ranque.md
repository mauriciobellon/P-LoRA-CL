# 2.3.2. Decomposição de baixo ranque

A decomposição de baixo ranque explora a hipótese de que as atualizações de pesos necessárias para uma nova tarefa podem ser representadas eficientemente em um subespaço de dimensão muito menor que o espaço original de parâmetros. Se W tem dimensões d×d, LoRA introduz duas matrizes A (d×r) e B (r×d), onde r << d. O produto AB resulta em uma matriz de atualização de posto no máximo r, permitindo representar mudanças complexas com muito menos parâmetros.

A inicialização das matrizes A e B é importante: tipicamente A é inicializada aleatoriamente e B é inicializada com zeros, garantindo que ΔW = 0 inicialmente e o modelo começa com o comportamento do modelo base. Durante o treinamento, apenas A e B são atualizados, mantendo W congelado.

# 2.3.2. Decomposição de baixo ranque

A decomposição de baixo ranque explora a hipótese de que as atualizações de pesos necessárias para uma nova tarefa podem ser representadas eficientemente em um subespaço de dimensão muito menor que o espaço original de parâmetros. Se W tem dimensões d×d, LoRA introduz duas matrizes A (d×r) e B (r×d), onde r << d. O produto AB resulta em uma matriz de atualização de posto no máximo r, permitindo representar mudanças complexas com muito menos parâmetros.

A inicialização das matrizes A e B é importante: tipicamente A é inicializada aleatoriamente e B é inicializada com zeros, garantindo que ΔW = 0 inicialmente e o modelo começa com o comportamento do modelo base. Durante o treinamento, apenas A e B são atualizados, mantendo W congelado.

Do ponto de vista geométrico, aprender A e B é equivalente a escolher uma base de r direções no espaço de parâmetros e aprender combinações lineares ao longo dessas direções. Extensões investigam alocação adaptativa de ranque por camada e por tarefa (ajustando r dinamicamente conforme a sensibilidade de cada módulo), bem como decomposições que operam no espaço de valores singulares (p. ex., parametrizando ΔW via SVD e realizando pruning dos singulares menos relevantes). Essas variações mantêm a filosofia central de expressividade controlada com eficiência paramétrica.

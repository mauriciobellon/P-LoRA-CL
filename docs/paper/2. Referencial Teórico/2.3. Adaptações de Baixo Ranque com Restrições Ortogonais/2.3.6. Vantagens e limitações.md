# 2.3.6. Vantagens e limitações

LoRA e suas variantes ortogonais oferecem um compromisso atraente entre isolamento de tarefas e eficiência. Cada tarefa é especializada por meio de um conjunto pequeno de parâmetros adicionais, resultando em crescimento linear modesto de memória com o número de tarefas (ordens de grandeza menor que adicionar colunas completas como na PNN). Com O-LoRA, obtém-se também isolamento efetivo entre tarefas, aproximando-se do ideal de "uma coluna por tarefa", porém de forma muito mais leve.

As limitações incluem o crescimento linear ainda presente (embora baixo), a necessidade de conhecimento da tarefa na inferência para ativar o conjunto correto de adaptadores, e possíveis dificuldades em manter ortogonalidade perfeita em sequências muito longas. Ainda assim, LoRA e O-LoRA representam avanços importantes para viabilizar aprendizado contínuo em modelos de PLN grandes, fornecendo eficiência paramétrica com isolamento suficiente para mitigar grande parte do esquecimento.

Do ponto de vista de implantação, surge o problema de roteamento de adapters: em cenários task-agnostic, um detector/roteador leve (por semelhança de embeddings, metadados ou cabeçotes dedicados) pode selecionar o adapter mais apropriado; alternativas incluem composição/fusão de adapters (AdapterFusion; Pfeiffer et al., 2021) para lidar com exemplos que acionam múltiplas competências. Em treinamento, LoRA ortogonal pode ser combinado com replay (real/gerativo) e regularizações leves (EWC/MAS/SI) para reforçar ainda mais a estabilidade, explorando sinergias entre mecanismos complementares.

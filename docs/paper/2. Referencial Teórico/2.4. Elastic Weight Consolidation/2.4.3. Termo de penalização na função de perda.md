# 2.4.3. Termo de penalização na função de perda

Matematicamente, a perda total do modelo ao aprender uma nova tarefa incorpora um termo do tipo:

L(θ) = L_novo(θ) + λ Σ_j (1/2) F_j (θ_j - θ_j*)^2

onde L_novo é a perda normal nos dados da nova tarefa, θ_j* é o valor do peso j após treinamento na tarefa anterior (mantido como referência), F_j é o elemento diagonal da matriz de Fisher, e λ é um hiperparâmetro que controla a força da penalização. Esse termo adicional atua como uma "mola" ancorando cada peso em torno do valor antigo com rigidez proporcional à importância F_j.

Em Online EWC, a referência θ* e a importância acumulada são atualizadas com fator de decaimento, prevenindo que o histórico remoto domine o comportamento do modelo e preservando plasticidade para tarefas recentes. Em arquiteturas com adapters, é comum aplicar EWC apenas ao backbone (congelado ou parcialmente ajustado) e deixar os adapters livres, obtendo proteção do conhecimento geral e especialização barata por tarefa.

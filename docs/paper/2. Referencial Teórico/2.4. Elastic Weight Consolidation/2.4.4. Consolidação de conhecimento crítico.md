# 2.4.4. Consolidação de conhecimento crítico

Pesos críticos ficam quase "congelados" (alta penalização se mudarem), enquanto pesos pouco relevantes podem se ajustar livremente à nova tarefa. Dessa forma, o EWC tenta obter um compromisso ótimo entre não esquecer o passado e ainda aprender o novo, encontrando uma região no espaço de parâmetros que minimize a perda da tarefa atual sem sair da região de bom desempenho das tarefas antigas.

A consolidação é particularmente efetiva quando aplicada a componentes compartilhados que mantêm conhecimento linguístico fundamental, como embeddings ou camadas iniciais do modelo. Essas camadas frequentemente codificam conhecimento geral que beneficia múltiplas tarefas, tornando-as candidatas ideais para proteção via EWC.

No contexto de PLN com grandes modelos, a combinação de EWC com adaptações leves (LoRA) fornece um caminho prático: congelar o backbone (ou permitir atualizações mínimas, protegidas por EWC) e concentrar plasticidade nos adapters. Essa estratégia reduz o risco de interferência destrutiva nas representações de base, enquanto permite especialização rápida por tarefa com baixo custo paramétrico.

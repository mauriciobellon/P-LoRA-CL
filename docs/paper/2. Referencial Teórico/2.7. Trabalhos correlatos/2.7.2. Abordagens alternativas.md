# 2.7.2. Abordagens alternativas

Existem também abordagens alternativas que não são diretamente combinadas neste trabalho, mas oferecem insights relevantes. HAT (Hard Attention to the Task) usa máscaras aprendidas para proteger parâmetros importantes, enquanto PackNet usa pruning para alocar subnetworks por tarefa. L2P e DualPrompt são métodos baseados em prompt tuning que armazenam prompts por tarefa em vez de adaptar pesos internos.

Essas abordagens alternativas demonstram a diversidade de estratégias disponíveis para aprendizado contínuo e destacam diferentes trade-offs entre isolamento, eficiência e flexibilidade. Embora não sejam diretamente incorporadas nesta proposta, oferecem perspectivas valiosas sobre possíveis extensões futuras.

Em mais detalhes: (i) HAT aprende máscaras binárias/duras por tarefa para bloquear atualizações em parâmetros críticos, reduzindo interferência sem crescimento paramétrico expressivo; (ii) PackNet realiza pruning iterativo para “liberar” capacidade que será reatribuída a novas tarefas; (iii) L2P (Wang et al., 2022) aprende um conjunto de prompts chave-valores e recupera um subconjunto para cada entrada, viabilizando CL sem rehearsal; (iv) DualPrompt estende a ideia com prompts gerais e específicos por tarefa, melhorando o roteamento de conhecimento. Essas linhas evidenciam que parametrização eficiente não se limita a adapters como LoRA; prompts e sparsity também constituem alavancas de controle de interferência.

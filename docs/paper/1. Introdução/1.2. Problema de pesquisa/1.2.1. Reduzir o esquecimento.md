# 1.2.1. Reduzir o esquecimento

Como reduzir o esquecimento catastrófico em sequências de tarefas de PLN sem armazenar dados brutos, mantendo eficiência paramétrica e custo de treino moderado?

Esta questão central orienta o desenvolvimento deste trabalho. O problema reconhece três restrições fundamentais: (i) a impossibilidade de armazenar dados brutos de tarefas anteriores, excluindo estratégias de replay baseadas em buffer real; (ii) a necessidade de eficiência paramétrica, limitando o crescimento do modelo com o número de tarefas; e (iii) a viabilidade computacional, exigindo que o custo de treinamento permaneça moderado mesmo com sequências longas de tarefas.

A combinação dessas restrições torna o problema particularmente desafiador. Métodos arquiteturais puros (como PNNs) oferecem isolamento completo, mas ao custo de crescimento linear de parâmetros. Métodos de regularização (como EWC) não requerem dados antigos, mas podem prejudicar a plasticidade em sequências longas. Métodos de replay gerativo evitam armazenamento de dados brutos, mas introduzem custo computacional adicional. A hipótese central deste trabalho é que uma combinação integrada dessas abordagens pode oferecer sinergias que mitigam suas limitações individuais, alcançando um equilíbrio superior entre estabilidade, plasticidade e eficiência.

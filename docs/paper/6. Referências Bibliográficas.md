# Referências Bibliográficas

ALJUNDI, R.; BABILONI, F.; ELHOSEINY, M.; ROHRBACH, M.; TUYTELAARS, T. Memory Aware Synapses: Learning what (not) to forget. In: European Conference on Computer Vision – ECCV 2018. Cham: Springer, 2018. p. 144–161. DOI: 10.1007/978-3-030-01219-9_9.

DE LANGE, M.; ALJUNDI, R.; MASANA, M.; PARISOT, S.; JIA, X.; LEONARDIS, A.; SLABAUGH, G.; TUYTELAARS, T. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, v. 44, n. 10, p. 3366–3385, 2022. DOI: 10.1109/TPAMI.2021.3057446.

DE MASSON D’AUTUME, C.; RUDER, S.; KONG, L.; YOGATAMA, D. Episodic Memory in Lifelong Language Learning. In: Advances in Neural Information Processing Systems (NeurIPS 2019). p. 13122–13131. Disponível em: https://papers.neurips.cc/paper/9471-episodic-memory-in-lifelong-language-learning.pdf
. Acesso em: 1 nov. 2025.

DETTMERS, T.; PAGNONI, A.; HOLTZMAN, A.; ZETTLEMOYER, L. QLoRA: Efficient Finetuning of Quantized LLMs. In: Advances in Neural Information Processing Systems (NeurIPS 2023). Disponível em: https://arxiv.org/abs/2305.14314
. Acesso em: 1 nov. 2025.

DEVLIN, J.; CHANG, M.-W.; LEE, K.; TOUTANOVA, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In: Proceedings of NAACL-HLT 2019. p. 4171–4186. DOI: 10.18653/v1/N19-1423.

FARAJTABAR, M.; AZIZAN, N.; MOTT, A.; LI, A. Orthogonal Gradient Descent for Continual Learning. arXiv:1910.07104, 2019. Disponível em: https://arxiv.org/abs/1910.07104
. Acesso em: 1 nov. 2025.

FRENCH, R. M. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences, v. 3, n. 4, p. 128–135, 1999. DOI: 10.1016/S1364-6613(99)01294-2.

GROSSBERG, S. Competitive learning: From interactive activation to adaptive resonance. Cognitive Science, v. 11, n. 1, p. 23–63, 1987. DOI: 10.1111/j.1551-6708.1987.tb00862.x.

HOULSBY, N.; GIURGIU, A.; JASTRZEBSKI, S.; MORRONE, B.; DE LAROUSSiLHE, Q.; GESMUNDO, A.; ATTARIYAN, M.; GELLY, S. Parameter-Efficient Transfer Learning for NLP. In: Proceedings of the 36th International Conference on Machine Learning (ICML 2019). PMLR 97, p. 2790–2799. Disponível em: https://proceedings.mlr.press/v97/houlsby19a.html
. Acesso em: 1 nov. 2025.

HU, E. J.; SHEN, Y.; WALLIS, P.; ALLEN-ZHU, Z.; LI, Y.; WANG, S.; WANG, L.; CHEN, W. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685, 2021. Disponível em: https://arxiv.org/abs/2106.09685
. Acesso em: 1 nov. 2025.

HUSZÁR, F. On Quadratic Penalties in Elastic Weight Consolidation. Proceedings of the National Academy of Sciences (PNAS), v. 115, n. 11, p. E2496–E2497, 2018. DOI: 10.1073/pnas.1717042115.

KIRKPATRICK, J.; PASCANU, R.; RABINOWITZ, N.; VENESS, J.; DESJARDINS, G.; RUSU, A.; MILAN, K.; QUAN, J.; RAMALHO, T.; GRABSKA-BARWINSKA, A.; HASSABIS, D.; CLOPATH, C.; KUMARAN, D.; HADSELL, R. Overcoming catastrophic forgetting in neural networks. PNAS, v. 114, n. 13, p. 3521–3526, 2017. DOI: 10.1073/pnas.1611835114.

LESTER, B.; AL-RFOU, R.; CONSTANT, N. The Power of Scale for Parameter-Efficient Prompt Tuning. In: Findings of EMNLP 2021. p. 3045–3059. DOI: 10.18653/v1/2021.findings-emnlp.265.

LI, X. L.; LIANG, P. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In: Proceedings of ACL 2021. p. 4582–4597. DOI: 10.18653/v1/2021.acl-long.353.

LI, Z.; HOIEM, D. Learning without Forgetting. In: European Conference on Computer Vision (ECCV 2016). Cham: Springer, 2016. p. 614–629. DOI: 10.1007/978-3-319-46493-0_37.

LOPEZ-PAZ, D.; RANZATO, M. Gradient Episodic Memory for Continual Learning. In: Advances in Neural Information Processing Systems (NeurIPS 2017). p. 6467–6476.

McCLOSKEY, M.; COHEN, N. J. Catastrophic interference in connectionist networks: The sequential learning problem. In: BOWER, G. H. (ed.). Psychology of Learning and Motivation, v. 24. San Diego: Academic Press, 1989. p. 109–165. DOI: 10.1016/S0079-7421(08)60137-8.

PARISI, G. I.; KEMKER, R.; PART, J. L.; KANAN, C.; WERMTER, S. Continual lifelong learning with neural networks: A review. Neural Networks, v. 113, p. 54–71, 2019. DOI: 10.1016/j.neunet.2019.01.012.

PFEIFFER, J.; KAMATH, A.; RÜCKLÉ, A.; CHO, K.; GUREVYCH, I. AdapterFusion: Non-Destructive Task Composition for Transfer Learning. In: Proceedings of the 16th Conference of the European Chapter of the ACL (EACL 2021). p. 312–318. DOI: 10.18653/v1/2021.eacl-main.39.

RUSU, A. A.; RABINOWITZ, N. C.; DESJARDINS, G.; SOYER, H.; KIRKPATRICK, J.; KAVUKCUOGLU, K.; PASCANU, R.; HADSELL, R. Progressive Neural Networks. arXiv:1606.04671, 2016. Disponível em: https://arxiv.org/abs/1606.04671
. Acesso em: 1 nov. 2025.

SANH, V.; DEBUT, L.; CHAUMOND, J.; WOLF, T. DistilBERT: A distilled version of BERT – smaller, faster, cheaper and lighter. arXiv:1910.01108, 2019. Disponível em: https://arxiv.org/abs/1910.01108
. Acesso em: 1 nov. 2025.

SCHWARZ, J.; LUKETINA, J.; CZARNECKI, W. M.; GRABSKA-BARWINSKA, A.; TEH, Y. W.; PASCANU, R.; HADSELL, R. Progress & Compress: A scalable framework for continual learning. In: Proceedings of the 35th International Conference on Machine Learning (ICML 2018). PMLR 80, p. 4528–4537. Disponível em: https://proceedings.mlr.press/v80/schwarz18a.html
. Acesso em: 1 nov. 2025.

SHIN, H.; LEE, J. K.; KIM, J.; KIM, J. Continual Learning with Deep Generative Replay. In: Advances in Neural Information Processing Systems (NeurIPS 2017). p. 2990–2999.

SUN, F.-K.; HO, C.-H.; LEE, H.-Y. LAMOL: Language Modeling for Lifelong Language Learning. In: International Conference on Learning Representations (ICLR 2020). Disponível em: https://openreview.net/forum?id=Skgxcn4YDS
. Acesso em: 1 nov. 2025.

VAN DE VEN, G. M.; TOLIAS, A. S. Three scenarios for continual learning. arXiv:1904.07734, 2019. Disponível em: https://arxiv.org/abs/1904.07734
. Acesso em: 1 nov. 2025.

VASWANI, A.; SHAZEER, N.; PARMAR, N.; USZKOREIT, J.; JONES, L.; GOMEZ, A. N.; KAISER, Ł.; POLOSUKHIN, I. Attention Is All You Need. In: Advances in Neural Information Processing Systems (NeurIPS 2017). p. 5998–6008.

ZENG, G.; CHEN, Y.; CUI, B.; YU, S. Continual Learning of Context-Dependent Processing in Neural Networks (OWM). Nature Machine Intelligence, v. 1, p. 364–372, 2019. DOI: 10.1038/s42256-019-0080-x.

ZHANG, X.; ZHAO, J.; LECUN, Y. Character-Level Convolutional Networks for Text Classification. In: Advances in Neural Information Processing Systems (NeurIPS 2015). p. 649–657.

ZENKE, F.; POOLE, B.; GANGULI, S. Continual Learning Through Synaptic Intelligence. In: Proceedings of the 34th International Conference on Machine Learning (ICML 2017). PMLR 70, p. 3987–3995. Disponível em: https://arxiv.org/abs/1703.04200
. Acesso em: 1 nov. 2025.

CHAUDHRY, A.; RANZATO, M.; ROHRBACH, M.; ELHOSEINY, M. Efficient Lifelong Learning with A-GEM. In: International Conference on Learning Representations (ICLR 2019). Disponível em: https://arxiv.org/abs/1812.00420
. Acesso em: 1 nov. 2025.

REBUFFI, S.-A.; KOLESNIKOV, A.; SPERL, G.; LAMPERT, C. H. iCaRL: Incremental Classifier and Representation Learning. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017). Disponível em: https://arxiv.org/abs/1611.07725
. Acesso em: 1 nov. 2025.

SERRÀ, J.; SURÍS, D.; MIRON, M.; KARATZOGLOU, A. Overcoming catastrophic forgetting with hard attention to the task. arXiv:1801.01423, 2018. Disponível em: https://arxiv.org/abs/1801.01423
. Acesso em: 1 nov. 2025.

WANG, Z.; ZHANG, Z.; LEE, C.-Y.; ZHANG, H.; SUN, R.; REN, X.; SU, G.; PEROT, V.; DY, J.; PFISTER, T. Learning to Prompt for Continual Learning. arXiv:2112.08654, 2021. Disponível em: https://arxiv.org/abs/2112.08654
. Acesso em: 1 nov. 2025.

WANG, Z.; ZHANG, Z.; EBRAHIMI, S.; SUN, R.; ZHANG, H.; LEE, C.-Y.; REN, X.; SU, G.; PEROT, V.; DY, J.; PFISTER, T. DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning. In: European Conference on Computer Vision (ECCV 2022). Disponível em: https://arxiv.org/abs/2204.04799
. Acesso em: 1 nov. 2025.

WANG, X.; ZHUANG, Z.; ZHANG, Y. PLAN: Proactive Low-Rank Allocation for Continual Learning. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV 2025), Honolulu, USA (aceito). arXiv:2510.21188. Disponível em: https://arxiv.org/abs/2510.21188
. Acesso em: 1 nov. 2025.

WOLF, T.; DEBERT, L.; SANH, V.; CHAUMOND, J.; DELANGUE, C.; MOI, A.; CISTAC, P.; RAULT, T.; LOUF, R.; FUNTOWICZ, M.; BRETAGNOLLE, L.; et al. Transformers: State-of-the-Art Natural Language Processing. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2020. DOI: 10.18653/v1/2020.emnlp-demos.6.

LOMONACO, V.; MALTONI, D.; SHE, Q.; CANGELOSI, A.; SHAPKINA, T.; EDAKKARAN, A.; et al. Avalanche: an End-to-End Library for Continual Learning. In: Proceedings of CVPR Workshops 2021. arXiv:2104.00405. Disponível em: https://arxiv.org/abs/2104.00405
. Acesso em: 1 nov. 2025.

MICIKEVICIUS, P.; NARANG, S.; ALBEN, J.; DUNGAN, P.; ELMORE, R.; GARCÍA, D.; GRAHAM, N.; et al. Mixed Precision Training. In: International Conference on Learning Representations (ICLR 2018) Workshop. arXiv:1710.03740. Disponível em: https://arxiv.org/abs/1710.03740
. Acesso em: 1 nov. 2025.

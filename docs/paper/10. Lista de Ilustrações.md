# 10. Lista de Ilustrações

## Figuras

### Capítulo 1: Introdução

**Figura 1.1: Dilema Estabilidade-Plasticidade no Aprendizado Contínuo**
- Localização: Seção 1.1.4
- Tipo: Diagrama conceitual
- Fonte: Criar (diagrama ilustrativo)
- Descrição: Diagrama mostrando o trade-off entre estabilidade (preservar conhecimento antigo) e plasticidade (aprender novo conhecimento), com eixos representando cada dimensão e uma zona ótima de equilíbrio. Ilustrar como diferentes métodos (fine-tuning puro, congelamento completo, abordagem híbrida) se posicionam nesse espaço.

**Figura 1.2: Sequência de Tarefas e Esquecimento Catastrófico**
- Localização: Seção 1.1.2
- Tipo: Gráfico de linha
- Fonte: Criar (exemplo ilustrativo)
- Descrição: Gráfico mostrando a evolução da acurácia em três tarefas ao longo do tempo, demonstrando como o desempenho nas tarefas anteriores degrada quando novas tarefas são aprendidas sem mecanismos de proteção.

### Capítulo 2: Referencial Teórico

**Figura 2.1: Arquitetura de Redes Neurais Progressivas (PNN)**
- Localização: Seção 2.2.1
- Tipo: Diagrama de arquitetura
- Fonte: Adaptar de Rusu et al. (2016)
- Descrição: Diagrama ilustrando a estrutura de PNN com múltiplas colunas (uma por tarefa), mostrando como cada nova tarefa adiciona uma coluna que recebe conexões laterais das colunas anteriores (congeladas).

**Figura 2.2: Decomposição LoRA de Baixo Ranque**
- Localização: Seção 2.3.2
- Tipo: Diagrama matemático
- Fonte: Adaptar de Hu et al. (2021)
- Descrição: Ilustração visual mostrando como uma matriz de peso W é mantida congelada e uma atualização de baixo ranque ΔW = AB é adicionada, com dimensões d×d para W, d×r para A, e r×d para B, destacando que r << d.

**Figura 2.3: Ortogonalidade entre Adaptadores LoRA**
- Localização: Seção 2.3.4
- Tipo: Diagrama geométrico
- Fonte: Criar (diagrama ilustrativo)
- Descrição: Representação geométrica mostrando subespaços ortogonais no espaço de parâmetros, com vetores representando direções de atualização de diferentes tarefas (T1, T2, T3) em ângulos aproximadamente perpendiculares, ilustrando o conceito de minimização de interferência.

**Figura 2.4: Elastic Weight Consolidation (EWC)**
- Localização: Seção 2.4.1
- Tipo: Diagrama conceitual
- Fonte: Adaptar de Kirkpatrick et al. (2017)
- Descrição: Visualização do landscape de perda mostrando como EWC adiciona "molas" (penalizações quadráticas) ancoradas nos valores ótimos da tarefa anterior, restringindo o movimento dos parâmetros importantes enquanto permite liberdade nos parâmetros menos críticos.

**Figura 2.5: Replay Gerativo no Aprendizado Contínuo**
- Localização: Seção 2.5.1
- Tipo: Diagrama de fluxo
- Fonte: Adaptar de Shin et al. (2017)
- Descrição: Fluxograma ilustrando o processo de replay gerativo: gerador produz exemplos sintéticos de tarefas anteriores → exemplos são intercalados com dados reais da tarefa atual → modelo é treinado na mistura → ciclo se repete para novas tarefas.

**Figura 2.6: Taxonomia de Métodos de Aprendizado Contínuo**
- Localização: Seção 2.1.1
- Tipo: Diagrama taxonômico
- Fonte: Adaptar de De Lange et al. (2021)
- Descrição: Árvore hierárquica classificando métodos de aprendizado contínuo em três famílias principais: (1) Baseados em Regularização (EWC, MAS, SI), (2) Baseados em Arquitetura (PNN, HAT, PackNet), (3) Baseados em Replay (GEM, A-GEM, gerativo), destacando onde a proposta híbrida se posiciona.

### Capítulo 3: Metodologia

**Figura 3.1: Arquitetura Híbrida Proposta**
- Localização: Seção 3.1
- Tipo: Diagrama de arquitetura
- Fonte: Gerar (visualize.py ou criar)
- Descrição: Diagrama completo da arquitetura integrando: (a) modelo base congelado, (b) adaptadores O-LoRA por tarefa, (c) aplicação de EWC em componentes compartilhados, (d) fluxo de replay gerativo, (e) conexões laterais opcionais. Usar cores diferentes para componentes congelados vs. treináveis.

**Figura 3.2: Fluxo de Treinamento Sequencial**
- Localização: Seção 3.3
- Tipo: Fluxograma
- Fonte: Criar (diagrama de processo)
- Descrição: Fluxograma detalhado do processo de treinamento sequencial mostrando: inicialização de adaptadores → treinamento com perda composta (L_task + L_ortho + L_ewc) → estimação de Fisher → avaliação cumulativa → próxima tarefa.

**Figura 3.3: Composição da Função de Perda**
- Localização: Seção 3.3.5
- Tipo: Diagrama de equação
- Fonte: Criar (visualização matemática)
- Descrição: Representação visual da função de perda composta L_total = L_task + λ_ortho·L_ortho + λ_ewc·L_ewc, mostrando cada componente e seus pesos relativos, com exemplos de valores típicos dos hiperparâmetros.

**Figura 3.4: Sequência de Tarefas Experimentais**
- Localização: Seção 3.2.1
- Tipo: Linha do tempo
- Fonte: Criar (diagrama ilustrativo)
- Descrição: Linha do tempo horizontal mostrando a sequência de cinco tarefas (AG News → Yelp Polarity → Amazon Reviews → DBPedia → Yahoo Answers) com características principais de cada dataset (domínio, número de classes, tamanho).

**Figura 3.5: Protocolo de Avaliação Cumulativa**
- Localização: Seção 3.5.1
- Tipo: Diagrama de matriz
- Fonte: Criar (diagrama ilustrativo)
- Descrição: Representação visual da matriz de desempenho R_{i,j}, mostrando como após cada tarefa i, o modelo é avaliado em todas as tarefas j≤i, com células coloridas indicando momentos de avaliação.

### Capítulo 4: Resultados e Discussão

**Figura 4.1: Evolução da Acurácia por Tarefa**
- Localização: Seção 4.1.1
- Tipo: Gráfico de linha (múltiplas séries)
- Fonte: Gerar com visualize.py (plot_accuracy_evolution)
- Descrição: Gráfico mostrando a evolução da acurácia de cada tarefa ao longo da sequência de treinamento. Eixo X = tarefa de treinamento atual, Eixo Y = acurácia, uma linha por tarefa avaliada. Permite visualizar quando e quanto cada tarefa sofre esquecimento.

**Figura 4.2: Comparação de Métodos - Métricas Agregadas**
- Localização: Seção 4.1.5
- Tipo: Gráfico de barras agrupadas
- Fonte: Gerar com visualize.py (plot_metrics_comparison)
- Descrição: Quatro subplots (ACC, BWT, FWT, Forgetting) comparando fine-tuning sequencial, LoRA sequencial, proposta híbrida, e joint training (upper bound). Barras coloridas por método.

**Figura 4.3: Matriz de Acurácia Final**
- Localização: Seção 4.1.2
- Tipo: Heatmap
- Fonte: Gerar com visualize.py (base nos dados da accuracy_matrix)
- Descrição: Heatmap da matriz R_{i,j} mostrando acurácia da tarefa j após treinar tarefa i. Diagonal principal mostra desempenho imediatamente após treinamento; última linha mostra desempenho final. Escala de cores indica magnitude da acurácia.

**Figura 4.4: Taxa de Esquecimento por Tarefa**
- Localização: Seção 4.1.3
- Tipo: Gráfico de barras
- Fonte: Gerar com visualize.py (baseado em forgetting metric)
- Descrição: Barras mostrando a taxa de esquecimento (A_max - A_final) para cada tarefa, comparando baseline vs. proposta. Barras negativas (ou ausentes) indicam ausência de esquecimento.

**Figura 4.5: Transferência Forward e Backward**
- Localização: Seção 4.1.4
- Tipo: Gráfico de barras duplas
- Fonte: Gerar com visualize.py (BWT e FWT)
- Descrição: Gráfico comparando BWT (transferência reversa) e FWT (transferência progressiva) entre diferentes métodos. BWT negativo indica esquecimento; FWT positivo indica transferência benéfica.

**Figura 4.6: Estudo de Ablação - Contribuição de Componentes**
- Localização: Seção 4.3.1
- Tipo: Gráfico de barras agrupadas
- Fonte: Gerar com visualize.py (plot_ablation)
- Descrição: Comparação de configurações ablacionadas: (a) Full (todos componentes), (b) Sem O-LoRA, (c) Sem EWC, (d) Sem Replay, (e) Sem Lateral. Métricas: ACC, BWT, FWT, Forgetting.

**Figura 4.7: Crescimento Paramétrico por Tarefa**
- Localização: Seção 4.2.1
- Tipo: Gráfico de linha/área empilhada
- Fonte: Gerar com visualize.py (dados de custos)
- Descrição: Gráfico mostrando crescimento cumulativo de parâmetros adicionais ao longo das tarefas. Comparar PNN completa (crescimento ~100% por tarefa) vs. LoRA/O-LoRA (crescimento ~1-2% por tarefa) vs. EWC (crescimento ~0% em parâmetros do modelo).

**Figura 4.8: Tempo de Treinamento e Uso de Memória**
- Localização: Seção 4.2.2
- Tipo: Gráfico de barras duplas
- Fonte: Gerar com visualize.py (dados de custos)
- Descrição: Dois subplots: (a) tempo de treinamento por tarefa para diferentes métodos, (b) pico de VRAM por método. Mostrar overhead de replay gerativo vs. métodos sem replay.

**Figura 4.9: Impacto do Replay Gerativo na Retenção**
- Localização: Seção 4.3.4
- Tipo: Gráfico de linha
- Fonte: Gerar com visualize.py (ablação de replay)
- Descrição: Comparação da acurácia final em cada tarefa com e sem replay gerativo, mostrando quanto o replay ajuda a preservar cada tarefa específica. Destacar tarefas que mais se beneficiam.

**Figura 4.10: Trade-off Estabilidade-Plasticidade na Prática**
- Localização: Seção 4.4.2
- Tipo: Scatter plot
- Fonte: Gerar com visualize.py (dados experimentais)
- Descrição: Scatter plot com ACC (plasticidade) no eixo X e -Forgetting (estabilidade) no eixo Y. Cada ponto representa uma configuração de hiperparâmetros. Mostrar como diferentes valores de λ_ewc e λ_ortho afetam o trade-off.

## Tabelas

### Capítulo 3: Metodologia

**Tabela 3.1: Características dos Datasets**
- Localização: Seção 3.2.1
- Tipo: Tabela descritiva
- Fonte: Dados dos datasets
- Descrição: Tabela com colunas: Dataset, Domínio, Número de Classes, Tamanho de Treino, Tamanho de Teste, Comprimento Médio de Texto. Linhas para AG News, Yelp, Amazon, DBPedia, Yahoo.

**Tabela 3.2: Hiperparâmetros Principais**
- Localização: Seção 3.3.6
- Tipo: Tabela de configuração
- Fonte: Configurações experimentais
- Descrição: Tabela listando hiperparâmetros principais: learning rate, batch size, LoRA rank (r), LoRA alpha, λ_ortho, λ_ewc, replay_ratio, epochs, warmup, weight decay, etc.

**Tabela 3.3: Configurações de Ablação**
- Localização: Seção 3.6.4
- Tipo: Tabela de configuração
- Fonte: Setup experimental
- Descrição: Matriz mostrando quais componentes estão ativos em cada configuração ablacionada: Full, -O-LoRA, -EWC, -Replay, -Lateral. Usar checkmarks (✓) e X para indicar presença/ausência.

### Capítulo 4: Resultados e Discussão

**Tabela 4.1: Matriz de Acurácia Completa (R_{i,j})**
- Localização: Seção 4.1.1
- Tipo: Tabela numérica
- Fonte: Gerar com visualize.py (generate_accuracy_matrix_table)
- Descrição: Matriz completa mostrando R_{i,j} (acurácia na tarefa j após treinar i tarefas). Linhas = etapa de treinamento (após tarefa i), Colunas = tarefa avaliada (j). Valores em porcentagem com 2 casas decimais.

**Tabela 4.2: Métricas Agregadas - Comparação de Métodos**
- Localização: Seção 4.1.5
- Tipo: Tabela comparativa
- Fonte: Gerar com visualize.py (generate_metrics_table)
- Descrição: Tabela comparando Fine-tuning, LoRA Seq., O-LoRA+EWC, O-LoRA+EWC+Replay (Full), Joint Training. Colunas: ACC ↑, BWT ↑, FWT ↑, Forgetting ↓. Incluir média ± desvio padrão de múltiplas seeds. Destacar melhor resultado em negrito.

**Tabela 4.3: Resultados de Ablação Detalhados**
- Localização: Seção 4.3.1
- Tipo: Tabela comparativa
- Fonte: Gerar com visualize.py (adaptado de metrics table)
- Descrição: Similar à Tabela 4.2, mas comparando configurações ablacionadas. Linhas: Full, -O-LoRA, -EWC, -Replay, -Lateral. Mostrar contribuição marginal de cada componente.

**Tabela 4.4: Custos Computacionais por Método**
- Localização: Seção 4.2
- Tipo: Tabela de eficiência
- Fonte: Dados experimentais de custos
- Descrição: Tabela com colunas: Método, Parâmetros Adicionais (total), Crescimento por Tarefa (%), Tempo de Treino (min), Pico VRAM (GB), Latência de Inferência (ms). Comparar todos os métodos.

**Tabela 4.5: Esquecimento por Tarefa - Comparação Detalhada**
- Localização: Seção 4.1.3
- Tipo: Tabela analítica
- Fonte: Calcular de accuracy_matrix
- Descrição: Tabela mostrando para cada tarefa: A_max (melhor acurácia obtida), A_final (acurácia após todas tarefas), Forgetting (diferença), comparando baseline vs. proposta. Destacar tarefas com maior/menor esquecimento.

**Tabela 4.6: Análise de Sensibilidade de Hiperparâmetros**
- Localização: Seção 4.4.2
- Tipo: Tabela experimental
- Fonte: Experimentos de busca
- Descrição: Tabela mostrando impacto de diferentes valores de λ_ewc e λ_ortho nas métricas principais. Linhas = combinações de hiperparâmetros, Colunas = ACC, BWT, FWT, Forgetting.

## Equações Destacadas

**Equação 3.1: Decomposição LoRA**
- Localização: Seção 3.1.3
- W' = W + ΔW = W + AB, onde A ∈ R^(d×r), B ∈ R^(r×d), r << d

**Equação 3.2: Perda Ortogonal**
- Localização: Seção 3.1.3
- L_ortho = Σ_layers Σ_{j<k} ||A_k @ A_j^T||²_F

**Equação 3.3: Perda EWC**
- Localização: Seção 3.3.5
- L_ewc = Σ_j (1/2) F_j (θ_j - θ*_j)²

**Equação 3.4: Função de Perda Composta**
- Localização: Seção 3.3.5
- L_total = L_task + λ_ortho·L_ortho + λ_ewc·L_ewc

**Equação 4.1: Average Accuracy (ACC)**
- Localização: Seção 3.5.4
- ACC = (1/N) Σ_{j=1}^N R_{N,j}

**Equação 4.2: Backward Transfer (BWT)**
- Localização: Seção 3.5.4
- BWT = (1/(N-1)) Σ_{i=1}^{N-1} (R_{N,i} - R_{i,i})

**Equação 4.3: Forward Transfer (FWT)**
- Localização: Seção 3.5.4
- FWT = (1/(N-1)) Σ_{i=2}^{N} (R_{i-1,i} - R_{0,i})

**Equação 4.4: Forgetting**
- Localização: Seção 3.5.4
- F_j = max_{k∈[1,N-1]} R_{k,j} - R_{N,j}

## Notas de Implementação

### Figuras Geradas Automaticamente (visualize.py)
- Figura 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 4.10
- Tabela 4.1, 4.2, 4.3, 4.4, 4.5

### Figuras a Criar Manualmente
- Figura 1.1, 1.2: Diagramas conceituais (usar draw.io, PowerPoint, ou Inkscape)
- Figura 2.1, 2.2, 2.3, 2.4, 2.5, 2.6: Adaptar de papers originais ou criar diagramas
- Figura 3.1, 3.2, 3.3, 3.4, 3.5: Diagramas de arquitetura e processo

### Figuras da Internet (com adaptação e citação)
- Figura 2.1: Adaptar de Rusu et al. (2016) - Progressive Neural Networks
- Figura 2.2: Adaptar de Hu et al. (2021) - LoRA paper
- Figura 2.4: Adaptar de Kirkpatrick et al. (2017) - EWC paper
- Figura 2.5: Adaptar de Shin et al. (2017) - Generative Replay
- Figura 2.6: Adaptar de De Lange et al. (2021) - Continual Learning Survey

### Recomendações de Ferramentas
- **Diagramas conceituais**: draw.io, Inkscape, TikZ (LaTeX)
- **Gráficos de dados**: matplotlib/seaborn (via visualize.py)
- **Diagramas de arquitetura**: draw.io, PlotNeuralNet (LaTeX)
- **Equações**: LaTeX
- **Tabelas**: pandas + LaTeX (via visualize.py)

### Formato e Resolução
- Figuras para paper: 300 DPI mínimo, formato PNG ou PDF
- Gráficos: usar cores colorblind-friendly (viridis, Set2)
- Tabelas: formato LaTeX para inclusão direta no documento
- Fontes: tamanho legível (≥10pt) mesmo quando figura é reduzida

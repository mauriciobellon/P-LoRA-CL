# 5.1.1. Resumo dos principais achados

Este trabalho investigou a viabilidade e efetividade de uma arquitetura híbrida para aprendizado contínuo em PLN que combina quatro mecanismos complementares: redes neurais progressivas (PNN) através de modularização leve, adaptações de baixo ranque com restrição ortogonal (O-LoRA), consolidação elástica de pesos (EWC), e replay gerativo parcimonioso. Quando executados, os experimentos deverão quantificar em que medida a integração desses mecanismos oferece proteção contra esquecimento catastrófico mantendo eficiência paramétrica e viabilidade computacional.

A análise empírica em sequência de cinco tarefas de classificação textual (AG News, Yelp Polarity, Amazon Reviews, DBPedia, Yahoo Answers) comparará a proposta a baselines de fine-tuning sequencial e LoRA sequencial, bem como a um upper bound de joint training, reportando ACC, BWT, FWT e crescimento paramétrico por tarefa.

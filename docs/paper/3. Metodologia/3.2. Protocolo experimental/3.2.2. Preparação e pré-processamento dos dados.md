# 3.2.2. Preparação e pré-processamento dos dados

Cada dataset é preparado seguindo práticas padrão de pré-processamento de texto. Removemos metadados irrelevantes, normalizamos espaços em branco e caracteres especiais, e garantimos que os textos estejam em formato adequado para o tokenizador do modelo base. Para tarefas de classificação multiclasse, mantemos todas as classes originais para maximizar a diversidade do desafio.

Os datasets são divididos em conjuntos de treino, validação e teste seguindo proporções padrão (tipicamente 70/15/15 ou conforme disponibilidade dos dados originais). É importante notar que seguimos um regime exemplar-free: após o treinamento em uma tarefa, não há acesso aos dados brutos dessa tarefa, exceto pelos exemplos sintéticos gerados para replay.

Higiene dos dados: removemos duplicatas estritas, entradas vazias e exemplos com comprimento fora de limites definidos (p. ex., truncamos textos acima de 512 tokens). Não realizamos lowercasing quando utilizamos tokenizadores sensíveis a caixa (BERT), preservando o pré-treino. Para fairness, mantemos os mesmos filtros para todos os métodos comparados e registramos estatísticas de pré-processamento (nº de exemplos removidos/truncados) por tarefa.

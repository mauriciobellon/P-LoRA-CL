# 3.2.3. Tokenização e configurações de comprimento máximo

A tokenização segue o tokenizador do modelo base (WordPiece para BERT). Configuramos comprimentos máximos específicos para cada dataset dentro da faixa de 256 a 512 tokens, balanceando capacidade de capturar contexto completo com eficiência computacional. Valores específicos são documentados em tabela para garantir reprodutibilidade.

Tokens especiais ([CLS], [SEP]) são adicionados conforme necessário pela arquitetura do modelo. Para tarefas que requerem pares de sequências, utilizamos o formato apropriado de separação. A tokenização é realizada uma vez antes do treinamento e os resultados são armazenados para evitar reprocessamento.

Usamos padding dinâmico por batch e ordenação aproximada por comprimento (bucketing) para reduzir padding médio e aumentar throughput. Em cenários com memória limitada, habilitamos truncamento agressivo (256 tokens) para datasets com textos longos, reportando os comprimentos efetivos adotados por tarefa.

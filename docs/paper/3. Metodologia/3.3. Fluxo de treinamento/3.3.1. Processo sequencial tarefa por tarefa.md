# 3.3.1. Processo sequencial tarefa por tarefa

O treinamento segue um protocolo estritamente sequencial: cada tarefa é treinada completamente antes de iniciar a próxima. Não há acesso a dados futuros durante o treinamento de uma tarefa atual, simulando um cenário realista onde tarefas chegam uma de cada vez ao longo do tempo.

Para cada tarefa T_k na sequência:
1. Avaliamos o modelo em todas as tarefas anteriores (T_1, ..., T_{k-1}) para medir desempenho atual antes de iniciar T_k
2. Inicializamos novos adaptadores LoRA para T_k
3. Congelamos adaptadores de tarefas anteriores (T_1, ..., T_{k-1})
4. Treinamos os novos adaptadores sobre T_k com perda composta (incluindo termos de ortogonalidade e EWC)
5. Intercalamos exemplos sintéticos de tarefas anteriores durante o treinamento
6. Após convergência, estimamos matriz de Fisher para EWC nas tarefas futuras
7. Avaliamos o modelo em todas as tarefas vistas até então (T_1, ..., T_k)

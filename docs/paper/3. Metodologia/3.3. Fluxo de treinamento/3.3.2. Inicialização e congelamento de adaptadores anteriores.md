# 3.3.2. Inicialização e congelamento de adaptadores anteriores

Novos adaptadores LoRA são inicializados seguindo a prática padrão: matrizes A são inicializadas aleatoriamente (distribuição normal pequena) e matrizes B são inicializadas com zeros, garantindo que ΔW = 0 inicialmente e o modelo começa com o comportamento do modelo base para a nova tarefa.

Adaptadores de tarefas anteriores são completamente congelados — seus parâmetros não são atualizados durante o treinamento da tarefa atual. Isso garante isolamento estrutural e previne interferência destrutiva. Os adaptadores congelados permanecem em memória e podem ser ativados durante a inferência quando necessário para a tarefa correspondente.

Para maximizar a estabilidade inicial, mantemos dropout de LoRA desabilitado nas primeiras épocas e, opcionalmente, aplicamos uma rampa de dropout leve posteriormente. Em cenários com conexões laterais, apenas os parâmetros de combinação/gating da tarefa corrente são treinados, mantendo estritamente congelados os caminhos de tarefas pretéritas.

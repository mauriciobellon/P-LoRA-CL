# 3.1.6. Integração de replay gerativo parcimonioso

O replay gerativo é implementado de forma parcimoniosa para minimizar custo computacional. A arquitetura utiliza um modelo gerador (que pode ser o próprio modelo base configurado para geração ou um modelo auxiliar) para produzir exemplos sintéticos das tarefas anteriores, seguindo a linha de Deep Generative Replay (Shin et al., 2017). Antes de cada batch de treinamento na tarefa atual T_k, o sistema pode gerar exemplos sintéticos representando as tarefas T_1, ..., T_{k-1}.

A integração no `CLTrainer` está preparada para consumir exemplos sintéticos: durante o loop de treinamento, quando `use_replay=True` e há tarefas anteriores, o trainer chama o `PseudoReplayGenerator` para gerar amostras que são intercaladas com dados reais, compondo tipicamente 10-30% de cada batch (controlado por `replay_ratio`).

**Nota de implementação:** A classe `PseudoReplayGenerator` está implementada com a estrutura completa de integração, mas o método `generate_samples` que produz o texto sintético é atualmente um placeholder. A lógica de geração usando o modelo em modo generativo (com prompts estruturados especificando tarefa e classe) precisa ser implementada para ativar completamente esta funcionalidade.

Quando completamente implementado, o sistema adotará: (i) quotas por classe/tarefa para manter balanceamento; (ii) filtros de qualidade mínima; (iii) hiperparâmetros de decodificação (temperature 0.7–1.0, top-p 0.9) calibrados; (iv) opção de congelar ou usar gerador dedicado para estabilizar distribuição, à semelhança do LAMOL (Sun et al., 2020). A arquitetura permite fácil extensão quando a geração for implementada, sem necessidade de refatoração do loop de treinamento.

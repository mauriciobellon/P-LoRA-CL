# 3.1.3. Adaptadores LoRA por tarefa com restrição ortogonal

Para cada nova tarefa T_k, inicializamos um novo conjunto de adaptadores LoRA que será treinado especificamente para essa tarefa (Hu et al., 2021). Os adaptadores são injetados nas projeções de atenção (Q, K, V, O) e/ou nas camadas feed-forward do modelo base, dependendo da configuração escolhida. Utilizamos ranks reduzidos (r = 4 a 8) para manter a eficiência paramétrica, resultando em overhead típico de 0,1% a 2% dos parâmetros do modelo base por tarefa.

Durante o treinamento dos adaptadores para T_k, impomos restrições ortogonais (O-LoRA) que garantem que os novos adaptadores ocupem subespaços distintos dos adaptadores de tarefas anteriores (T_1, ..., T_{k-1}). Isso é feito através de um termo de regularização na função de perda que penaliza projeções dos novos adaptadores nos subespaços gerados pelos adaptadores anteriores, minimizando interferência entre tarefas (inspirado em OWM/OGD; Zeng et al., 2019; Farajtabar et al., 2019).

Concretamente, denotando A_k as colunas da base de baixo ranque da tarefa k (e A_<k a união de bases anteriores), adotamos: L_ortho = Σ_i ||Proj_{span(A_<k)}(a_{k,i})||², onde a_{k,i} é uma coluna de A_k. Alternativamente, utilizamos penalização de correlação/cosseno entre direções de A_k e A_<k. Em tarefas com maior conflito, aplicamos projeção tipo Gram–Schmidt após cada atualização para reforçar ortogonalidade. Um agendamento crescente de λ_ortho ao longo das épocas ajuda a preservar plasticidade no início e aumentar isolamento próximo da convergência. Trabalhos recentes investigam alocação proativa de bases com mínima interferência (PLAN; Wang, Zhuang & Zhang, 2025), alinhados com esse princípio.

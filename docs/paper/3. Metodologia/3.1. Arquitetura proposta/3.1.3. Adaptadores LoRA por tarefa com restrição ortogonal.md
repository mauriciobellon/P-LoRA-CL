# 3.1.3. Adaptadores LoRA por tarefa com restrição ortogonal

Para cada nova tarefa T_k, criamos um novo `PeftModel` contendo um conjunto de adaptadores LoRA que será treinado especificamente para essa tarefa (Hu et al., 2021). Cada tarefa recebe seu próprio `PeftModel` completo, garantindo isolamento total entre adaptadores de diferentes tarefas e evitando conflitos ao modificar o modelo base múltiplas vezes. Os adaptadores são injetados nas projeções de atenção (Q, K, V, O) do modelo base, com detecção automática dos módulos alvo baseada na arquitetura. Utilizamos ranks reduzidos (r = 4 a 8) para manter eficiência paramétrica, resultando em overhead típico de 0,1% a 2% dos parâmetros do modelo base por tarefa.

Durante o treinamento dos adaptadores para T_k, impomos restrições ortogonais (O-LoRA) que garantem que os novos adaptadores ocupem subespaços distintos dos adaptadores de tarefas anteriores (T_1, ..., T_{k-1}). Isso é implementado através de um termo de regularização na função de perda que penaliza projeções dos novos adaptadores nos subespaços gerados pelos adaptadores anteriores, minimizando interferência entre tarefas (inspirado em OWM/OGD; Zeng et al., 2019; Farajtabar et al., 2019).

Na implementação, armazenamos os `PeftModel` de cada tarefa anterior e, durante o treinamento de T_k, extraímos as matrizes LoRA (A e B) de tarefas anteriores para calcular a perda ortogonal. Concretamente, aproximamos a projeção através de `||A_k @ A_{prev}.T||²` para cada par de adaptadores correspondentes. O peso da regularização ortogonal (λ_ortho, tipicamente 0.1) é configurável e balanceia a preservação de conhecimento anterior com a plasticidade para a nova tarefa.

A arquitetura de gerenciamento de adaptadores (`LoRAAdapterManager` e `OrthogonalLoRA`) mantém um dicionário de modelos PEFT por tarefa, permitindo ativação rápida do adaptador apropriado durante treinamento ou avaliação. Cada adaptador é congelado após o treinamento de sua tarefa, garantindo que tarefas futuras não modifiquem conhecimento adquirido anteriormente.

# 3.1.1. Modelo base e configuração

A arquitetura proposta utiliza modelos base de porte moderado para garantir viabilidade computacional com recursos limitados. Especificamente, empregamos BERT-base (110M parâmetros) ou DistilBERT (66M parâmetros) como modelos base pré-treinados (Devlin et al., 2019; Sanh et al., 2019). Esses modelos oferecem um bom equilíbrio entre capacidade de representação e eficiência computacional, sendo amplamente utilizados em benchmarks de aprendizado contínuo.

O modelo base é mantido majoritariamente congelado durante todo o processo de aprendizado contínuo, com apenas componentes específicos sendo parcialmente destravados para aplicação de EWC. A cabeça de classificação padrão (um classificador linear sobre a representação [CLS]) é mantida genérica e pode ser adaptada por tarefa através dos adaptadores LoRA (Hu et al., 2021). Esta configuração permite que o modelo compartilhe conhecimento linguístico fundamental enquanto especializa-se para tarefas específicas através de módulos leves.

Para evitar conflito de rótulos entre tarefas com espaços de classes distintos, utilizamos cabeças de classificação específicas por tarefa no cenário task-aware (3.5.2). Ou seja, a arquitetura da cabeça é compartilhada, mas os pesos do último layer (logits) são instanciados por tarefa. Em setups com tarefas binárias (e.g., sentimento) e multiclasse (e.g., DBPedia), isso impede colisão de rótulos e simplifica a avaliação. Opcionalmente, em ambientes com restrições de memória, um único cabeçote compartilhado pode ser usado com mapeamentos por tarefa, mas adotamos cabeças por tarefa para clareza experimental.

Quando disponível, avaliamos também uma variação com quantização do backbone (QLoRA; Dettmers et al., 2023), mantendo o backbone congelado em 4/8 bits e treinando apenas adapters em maior precisão. Essa configuração reduz VRAM sem alterar a lógica do arranjo e é especialmente útil em GPUs de 16GB.

# 3.1.1. Modelo base e configuração

A arquitetura proposta utiliza modelos base de porte moderado para garantir viabilidade computacional com recursos limitados. Especificamente, empregamos BERT-base (110M parâmetros) ou DistilBERT (66M parâmetros) como modelos base pré-treinados (Devlin et al., 2019; Sanh et al., 2019). Esses modelos oferecem um bom equilíbrio entre capacidade de representação e eficiência computacional, sendo amplamente utilizados em benchmarks de aprendizado contínuo.

O modelo base é carregado sem cabeça de classificação pré-construída (usando `AutoModel` da biblioteca Transformers), mantendo apenas o backbone transformer. Este é congelado durante todo o processo de aprendizado contínuo, com exceção dos adaptadores LoRA que são aplicados nas camadas de atenção. A abordagem garante que o conhecimento linguístico fundamental permaneça estável enquanto permitimos especialização por tarefa através dos adaptadores.

Para cada tarefa, criamos uma cabeça de classificação linear independente (mapeando a representação [CLS] para o espaço de classes da tarefa) que é treinada juntamente com os adaptadores LoRA da tarefa. Esta arquitetura evita conflitos de rótulos entre tarefas com espaços de classes distintos no cenário task-aware (3.5.2). Em setups com tarefas binárias (e.g., sentimento) e multiclasse (e.g., DBPedia), isso impede colisão de rótulos e simplifica a avaliação.

A implementação detecta automaticamente os nomes dos módulos alvo para LoRA com base na arquitetura do modelo (e.g., `q_lin`, `k_lin`, `v_lin`, `out_lin` para DistilBERT; `query`, `key`, `value`, `dense` para BERT padrão), garantindo compatibilidade com diferentes arquiteturas de transformers sem necessidade de configuração manual.

Quando disponível, a arquitetura suporta também quantização do backbone (QLoRA; Dettmers et al., 2023), mantendo o backbone em 4/8 bits e treinando adaptadores em maior precisão, reduzindo uso de VRAM sem alterar a lógica do arranjo.

# 3.1.1. Modelo base e configuração

A arquitetura proposta utiliza modelos base de porte moderado para garantir viabilidade computacional com recursos limitados. Especificamente, empregamos BERT-base (110M parâmetros) ou DistilBERT (66M parâmetros) como modelos base pré-treinados (Devlin et al., 2019; Sanh et al., 2019). Esses modelos oferecem um bom equilíbrio entre capacidade de representação e eficiência computacional, sendo amplamente utilizados em benchmarks de aprendizado contínuo.

O modelo base é mantido majoritariamente congelado durante todo o processo de aprendizado contínuo, com apenas componentes específicos sendo parcialmente destravados para aplicação de EWC. A cabeça de classificação padrão (um classificador linear sobre a representação [CLS]) é mantida genérica e pode ser adaptada por tarefa através dos adaptadores LoRA (Hu et al., 2021). Esta configuração permite que o modelo compartilhe conhecimento linguístico fundamental enquanto especializa-se para tarefas específicas através de módulos leves.

# 3.1.4. Conexões laterais opcionais para transferência

Para promover transferência positiva entre tarefas, implementamos conexões laterais opcionais inspiradas em PNN (Rusu et al., 2016). Essas conexões permitem que o módulo atual (adaptadores da tarefa corrente) consuma representações dos módulos anteriores (adaptadores de tarefas passadas) sem atualizá-los. As conexões podem ser implementadas através de concatenação de features, soma ponderada, ou mecanismos de atenção que aprendem a combinar informações de diferentes adaptadores.

As conexões laterais são configuráveis e podem ser habilitadas ou desabilitadas para análise de ablação, permitindo quantificar seu impacto na transferência forward e no desempenho geral. Quando habilitadas, elas adicionam um pequeno overhead computacional mas podem melhorar significativamente o desempenho inicial em novas tarefas através de aproveitamento de conhecimento prévio.

Para evitar transferência negativa, aplicamos normalização e gating aprendível por camada, permitindo ao modelo atenuar contribuições de tarefas pouco relacionadas. Em Transformers, a opção adotada é um bloco de atenção cruzada leve entre a representação corrente e caches de saídas intermediárias de adaptadores antigos. Pesos dessas conexões são treinados apenas para a tarefa corrente, mantendo módulos antigos congelados. Essa escolha preserva o princípio de não interferência direta em tarefas anteriores.

# 3.1.4. Conexões laterais opcionais para transferência

Para promover transferência positiva entre tarefas, a arquitetura prevê conexões laterais opcionais inspiradas em PNN (Rusu et al., 2016). Essas conexões permitiriam que o módulo atual (adaptadores da tarefa corrente) consuma representações dos módulos anteriores (adaptadores de tarefas passadas) sem atualizá-los. As conexões podem ser implementadas através de concatenação de features, soma ponderada, ou mecanismos de atenção que aprendem a combinar informações de diferentes adaptadores.

**Nota de implementação:** A flag `use_lateral` está presente na interface do trainer e nos arquivos de configuração, permitindo habilitar ou desabilitar as conexões para análise de ablação. Contudo, a lógica específica das conexões laterais no método `forward` do modelo e no loop de treinamento ainda não foi implementada na versão atual. Esta funcionalidade está planejada como extensão futura para quantificar seu impacto na transferência forward (FWT) e no desempenho geral.

Quando implementadas, as conexões laterais adicionarão um pequeno overhead computacional mas poderão melhorar significativamente o desempenho inicial em novas tarefas através de aproveitamento de conhecimento prévio. Para evitar transferência negativa, planejamos aplicar normalização e gating aprendível por camada, permitindo ao modelo atenuar contribuições de tarefas pouco relacionadas. Em Transformers, a opção natural seria um bloco de atenção cruzada leve entre a representação corrente e caches de saídas intermediárias de adaptadores antigos, com pesos treinados apenas para a tarefa corrente mantendo módulos antigos congelados.

# 3.1.2. Estrutura modular inspirada em PNN

A arquitetura incorpora princípios de modularização progressiva inspirados em PNN (Rusu et al., 2016), mas de forma parametricamente eficiente. Em vez de adicionar colunas completas de rede para cada tarefa, adicionamos apenas conjuntos de adaptadores LoRA leves. Cada tarefa recebe seu próprio conjunto de adaptadores que são congelados após o treinamento dessa tarefa, criando isolamento estrutural similar ao das PNNs, mas com crescimento paramétrico muito menor.

A estrutura modular permite que adaptadores de tarefas anteriores sejam mantidos em memória e ativados durante a inferência conforme necessário. Essa abordagem mantém a propriedade de isolamento completa das PNNs (adaptadores anteriores não são atualizados durante treinamento de novas tarefas) enquanto reduz drasticamente o custo de armazenamento e computação.

Implementação: os adaptadores por tarefa são injetados de forma homogênea nos mesmos pontos da arquitetura (projeções de atenção e/ou MLP), permitindo comparação justa entre tarefas. Os módulos são nomeados com prefixos por tarefa (p. ex., t1_attn_q, t1_ffn_up) e armazenados em um registry para ativação condicionada pelo ID de tarefa. Caso conexões laterais estejam habilitadas (3.1.4), os outputs intermediários dos adaptadores antigos são disponibilizados via hooks, com mecanismos de atenção leve para combinar representações sem atualizar módulos congelados.

# 3.4.1. Framework e bibliotecas

A implementação utiliza PyTorch como framework principal de deep learning, aproveitando sua flexibilidade para implementar componentes customizados. HuggingFace Transformers fornece modelos base pré-treinados e utilitários de tokenização, enquanto PEFT (Parameter-Efficient Fine-Tuning) oferece implementações otimizadas de LoRA e gerenciamento de adaptadores.

A implementação customizada não utiliza Avalanche, optando por uma arquitetura própria mais leve e especializada para os requisitos específicos do trabalho. O gerenciamento de sequências de tarefas é implementado em `src/plora_cl/data/datasets.py`, as métricas de continual learning em `src/plora_cl/evaluation/metrics.py`, e o loop de treinamento em `src/plora_cl/training/trainer.py`. EWC é implementado diretamente em `src/plora_cl/models/ewc.py` com cálculo customizado da matriz de Fisher. Componentes adicionais incluem O-LoRA (`orthogonal_lora.py`), estrutura de replay gerativo (`replay.py`), e sistema de tracking de experimentos (`evaluation/tracker.py`).

A implementação inclui logging extensivo com saída em tempo real (`flush=True`), permitindo monitoramento detalhado do progresso durante treinamento, especialmente útil para experimentos longos em CPU. O sistema registra configuração inicial, progresso por tarefa/época/batch, métricas de loss, tempo de treinamento e uso de VRAM.

Versões e reprodutibilidade: utilizamos versões estáveis de PyTorch (≥2.0.0) e Transformers (≥4.30.0). Fixamos seeds de PyTorch/NumPy/Python via configuração no trainer. Scripts de execução via CLI (`uv run python -m plora_cl.cli.train`) registram todas as configurações em arquivos JSON, permitindo reprodução exata dos experimentos. Gerenciamento de dependências via `pyproject.toml` (PEP 518/621) com `uv` para ambientes reprodutíveis.

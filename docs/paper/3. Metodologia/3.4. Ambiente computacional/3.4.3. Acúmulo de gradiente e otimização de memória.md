# 3.4.3. Acúmulo de gradiente e otimização de memória

Para otimizar uso de memória e permitir batch sizes efetivos maiores, utilizamos acúmulo de gradiente (gradient accumulation). Isso permite simular batch sizes maiores sem aumentar proporcionalmente o uso de VRAM, dividindo o batch em múltiplos micro-batches e acumulando gradientes antes de atualizar parâmetros.

Outras otimizações de memória incluem: remoção de gradientes não utilizados através de torch.no_grad() quando apropriado, uso de tipos de dados eficientes (float16 onde possível), e carregamento eficiente de dados através de DataLoader com num_workers otimizado.

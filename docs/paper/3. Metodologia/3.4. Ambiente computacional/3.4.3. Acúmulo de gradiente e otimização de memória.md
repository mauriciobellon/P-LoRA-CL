# 3.4.3. Acúmulo de gradiente e otimização de memória

Para otimizar uso de memória e permitir batch sizes efetivos maiores, utilizamos acúmulo de gradiente (gradient accumulation). Isso permite simular batch sizes maiores sem aumentar proporcionalmente o uso de VRAM, dividindo o batch em múltiplos micro-batches e acumulando gradientes antes de atualizar parâmetros.

Outras otimizações de memória incluem: remoção de gradientes não utilizados através de torch.no_grad() quando apropriado, uso de tipos de dados eficientes (float16 onde possível), e carregamento eficiente de dados através de DataLoader com num_workers otimizado.

Também adotamos ativação de atenção eficiente quando disponível, desabilitamos criação de gráficos desnecessários em passagens de validação e utilizamos pin_memory e prefetch para aumentar throughput de E/S. Em cenários com geração frequente, cacheamos prompts e outputs para reduzir chamadas redundantes ao gerador.

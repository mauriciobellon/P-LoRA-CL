# 3.4.2. Configuração de hardware

Os experimentos são projetados para execução em uma única GPU intermediária (por exemplo, NVIDIA T4 com 16GB VRAM), garantindo viabilidade para contextos acadêmicos com recursos limitados. Utilizamos precisão mista (mixed precision) através de torch.cuda.amp para reduzir uso de memória e acelerar treinamento, permitindo batch sizes maiores e reduzindo tempo de treinamento.

Gradiente checkpointing é aplicado quando necessário para modelos maiores, trocando computação por memória e permitindo processar sequências mais longas ou batches maiores dentro das limitações de VRAM disponível.

Quando aplicável, empregamos QLoRA para quantizar o backbone e reduzir VRAM mantendo adapters em maior precisão (Dettmers et al., 2023). Mixed precision segue práticas de Micikevicius et al. (2018), com escalonamento de perda automático e monitoração de under/overflow. Reportamos picos de VRAM e tempos por tarefa (3.5.5).

# 3.4.2. Configuração de hardware

Os experimentos são projetados para execução tanto em CPU quanto em GPU, garantindo acessibilidade máxima. O sistema suporta execução em CPU (validado durante desenvolvimento), GPU única intermediária (por exemplo, NVIDIA T4 com 16GB VRAM), ou detecção automática via flag `--device auto`.

A implementação atual foca em compatibilidade e correção, validada primeiramente em CPU. Otimizações para GPU incluem suporte futuro a precisão mista (mixed precision) através de `torch.cuda.amp` para reduzir uso de memória e acelerar treinamento, permitindo batch sizes maiores.

Gradiente checkpointing pode ser aplicado quando necessário para modelos maiores, trocando computação por memória e permitindo processar sequências mais longas ou batches maiores dentro das limitações de VRAM disponível. QLoRA está planejado para quantizar o backbone e reduzir VRAM mantendo adapters em maior precisão (Dettmers et al., 2023).

O sistema reporta automaticamente picos de VRAM (quando disponível) e tempos de treinamento por tarefa através do `ExperimentTracker` (3.5.5). Durante treinamento, o progresso é monitorado em tempo real com informações de epoch, batch, e métricas de loss.

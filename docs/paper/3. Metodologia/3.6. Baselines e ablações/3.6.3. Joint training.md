# 3.6.3. Joint training

Como upper bound teórico, treinamos o modelo simultaneamente em todas as tarefas com acesso completo a todos os dados (joint training). Esta abordagem não é viável em cenários de aprendizado contínuo real, mas fornece referência do desempenho máximo possível se não houvesse restrições de dados e tempo.

A diferença entre joint training e os métodos de aprendizado contínuo quantifica o custo de aprender sequencialmente versus simultaneamente, e permite avaliar quão próximos os métodos propostos estão do limite teórico.

Configuração: mistura balanceada de todas as tarefas, com splits originais e sem restrições de memória/dados. Usamos os mesmos hiperparâmetros globais e treinamos até convergência em validação global. Este resultado não é comparável diretamente em termos de custo, mas serve como referência de desempenho máximo.

**Nota de implementação:** A classe `JointTrainingTrainer` em `src/plora_cl/training/baselines.py` está implementada como estrutura básica que carrega todos os datasets. A lógica completa de mistura balanceada de batches e treinamento conjunto está planejada para implementação futura. A arquitetura permite fácil extensão sem necessidade de modificação do framework principal.

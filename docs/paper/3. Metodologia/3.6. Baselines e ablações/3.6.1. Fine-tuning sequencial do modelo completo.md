# 3.6.1. Fine-tuning sequencial do modelo completo

Como baseline representativo de esquecimento catastrófico sem mitigação, treinamos o modelo completo (todos os parâmetros) sequencialmente em cada tarefa sem nenhuma proteção contra esquecimento. Esta abordagem serve como lower bound e demonstra a severidade do problema de esquecimento catastrófico no contexto estudado.

Esperamos observar degradação significativa do desempenho em tarefas anteriores à medida que novas tarefas são aprendidas, fornecendo linha de base para comparar a efetividade das técnicas propostas.

Configuração: mesmo otimizador/scheduler dos demais métodos, sem EWC/replay/ortogonalidade. Para justiça, mantemos número de épocas/steps e critérios de early stopping idênticos por tarefa. Este baseline tipicamente apresenta BWT negativo pronunciado e forgetting alto.

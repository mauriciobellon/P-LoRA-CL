# 3.6.2. LoRA único sequencial

Como baseline intermediário, treinamos um único conjunto de adaptadores LoRA (sem ortogonalidade) sequencialmente reutilizado para todas as tarefas. Esta abordagem demonstra a eficiência paramétrica do LoRA mas também mostra que LoRA puro não resolve esquecimento quando usado sequencialmente.

Comparação com este baseline permite quantificar o valor adicional da ortogonalidade (O-LoRA) em reduzir interferência entre tarefas.

Configuração: um único conjunto de adapters LoRA é reaproveitado e atualizado a cada tarefa (sem congelamento por tarefa), mantendo o backbone congelado. Espera-se melhor eficiência paramétrica em relação ao fine-tuning completo, porém com esquecimento relevante devido à reutilização do mesmo subespaço de atualização.
